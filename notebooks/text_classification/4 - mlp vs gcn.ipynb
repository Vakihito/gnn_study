{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO305k1rJDLJ2lK1yyFuc+F"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"WLhi2BH4k7dD","executionInfo":{"status":"ok","timestamp":1717529004137,"user_tz":180,"elapsed":17362,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"outputs":[],"source":["%%capture\n","!pip install torch-geometric"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gjEw8zVlGpI","executionInfo":{"status":"ok","timestamp":1717529199061,"user_tz":180,"elapsed":21463,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"44ed8f21-e455-41ef-b244-7367425b1f48"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["node_path = '/content/drive/MyDrive/node_classification/text_classification'"],"metadata":{"id":"HahTPWIYlHsX","executionInfo":{"status":"ok","timestamp":1717529199062,"user_tz":180,"elapsed":4,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from torch_geometric.data import Data\n","import numpy as np\n","import torch"],"metadata":{"id":"kpvzi3D6lPTk","executionInfo":{"status":"ok","timestamp":1717529205162,"user_tz":180,"elapsed":6102,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"6qdYIEKCsu5I","executionInfo":{"status":"ok","timestamp":1717529205163,"user_tz":180,"elapsed":36,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["dataset = torch.load(f\"{node_path}/data/ptg_complete_data.pth\")\n","dataset.num_classes = len(torch.unique(dataset.y))"],"metadata":{"id":"lc9yj5F4lQhx","executionInfo":{"status":"ok","timestamp":1717529210392,"user_tz":180,"elapsed":5261,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["dataset = dataset.to(device)"],"metadata":{"id":"8cCjzwNTsxSr","executionInfo":{"status":"ok","timestamp":1717529211000,"user_tz":180,"elapsed":618,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpDsdCptlWsj","executionInfo":{"status":"ok","timestamp":1717529211000,"user_tz":180,"elapsed":38,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"52d3b9a4-2418-4df8-ee7e-133758b06d27"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Data(x=[50000, 384], edge_index=[2, 613827], y=[50000], num_classes=2, train_mask=[50000], val_mask=[50000], test_mask=[50000])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["dataset"],"metadata":{"id":"-WYcwNmVuKt0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.nn import Linear\n","import torch.nn.functional as F\n","\n","\n","class MLP(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(12345)\n","        self.lin1 = Linear(dataset.num_features, hidden_channels)\n","        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x):\n","        x = self.lin1(x)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin2(x)\n","        return x\n","\n","model = MLP(hidden_channels=16)\n","model = model.to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RN00UP6Ylwdi","executionInfo":{"status":"ok","timestamp":1717529211001,"user_tz":180,"elapsed":34,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"4d7c67b9-81aa-4020-aa61-71104f422143"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["MLP(\n","  (lin1): Linear(in_features=384, out_features=16, bias=True)\n","  (lin2): Linear(in_features=16, out_features=2, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["data = dataset"],"metadata":{"id":"0UBL13E6qa3q","executionInfo":{"status":"ok","timestamp":1717529211001,"user_tz":180,"elapsed":27,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Javascript  # Restrict height of output cell.\n","\n","model = MLP(hidden_channels=16)\n","model = model.to(device)\n","criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","for epoch in range(1, 201):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YvSIJF1smYm6","executionInfo":{"status":"ok","timestamp":1717529347799,"user_tz":180,"elapsed":3008,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"a217c520-022f-406a-f7b9-1a8252eaceaf"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 0.6982\n","Epoch: 002, Loss: 0.6897\n","Epoch: 003, Loss: 0.6817\n","Epoch: 004, Loss: 0.6729\n","Epoch: 005, Loss: 0.6622\n","Epoch: 006, Loss: 0.6482\n","Epoch: 007, Loss: 0.6351\n","Epoch: 008, Loss: 0.6219\n","Epoch: 009, Loss: 0.6095\n","Epoch: 010, Loss: 0.5944\n","Epoch: 011, Loss: 0.5787\n","Epoch: 012, Loss: 0.5684\n","Epoch: 013, Loss: 0.5565\n","Epoch: 014, Loss: 0.5476\n","Epoch: 015, Loss: 0.5364\n","Epoch: 016, Loss: 0.5284\n","Epoch: 017, Loss: 0.5235\n","Epoch: 018, Loss: 0.5176\n","Epoch: 019, Loss: 0.5125\n","Epoch: 020, Loss: 0.5043\n","Epoch: 021, Loss: 0.5041\n","Epoch: 022, Loss: 0.5012\n","Epoch: 023, Loss: 0.4972\n","Epoch: 024, Loss: 0.4940\n","Epoch: 025, Loss: 0.4896\n","Epoch: 026, Loss: 0.4883\n","Epoch: 027, Loss: 0.4856\n","Epoch: 028, Loss: 0.4840\n","Epoch: 029, Loss: 0.4826\n","Epoch: 030, Loss: 0.4800\n","Epoch: 031, Loss: 0.4775\n","Epoch: 032, Loss: 0.4774\n","Epoch: 033, Loss: 0.4770\n","Epoch: 034, Loss: 0.4745\n","Epoch: 035, Loss: 0.4743\n","Epoch: 036, Loss: 0.4720\n","Epoch: 037, Loss: 0.4699\n","Epoch: 038, Loss: 0.4696\n","Epoch: 039, Loss: 0.4683\n","Epoch: 040, Loss: 0.4705\n","Epoch: 041, Loss: 0.4679\n","Epoch: 042, Loss: 0.4689\n","Epoch: 043, Loss: 0.4678\n","Epoch: 044, Loss: 0.4671\n","Epoch: 045, Loss: 0.4664\n","Epoch: 046, Loss: 0.4643\n","Epoch: 047, Loss: 0.4642\n","Epoch: 048, Loss: 0.4630\n","Epoch: 049, Loss: 0.4658\n","Epoch: 050, Loss: 0.4636\n","Epoch: 051, Loss: 0.4651\n","Epoch: 052, Loss: 0.4639\n","Epoch: 053, Loss: 0.4627\n","Epoch: 054, Loss: 0.4642\n","Epoch: 055, Loss: 0.4621\n","Epoch: 056, Loss: 0.4615\n","Epoch: 057, Loss: 0.4617\n","Epoch: 058, Loss: 0.4596\n","Epoch: 059, Loss: 0.4643\n","Epoch: 060, Loss: 0.4612\n","Epoch: 061, Loss: 0.4610\n","Epoch: 062, Loss: 0.4595\n","Epoch: 063, Loss: 0.4611\n","Epoch: 064, Loss: 0.4595\n","Epoch: 065, Loss: 0.4600\n","Epoch: 066, Loss: 0.4617\n","Epoch: 067, Loss: 0.4592\n","Epoch: 068, Loss: 0.4599\n","Epoch: 069, Loss: 0.4586\n","Epoch: 070, Loss: 0.4595\n","Epoch: 071, Loss: 0.4595\n","Epoch: 072, Loss: 0.4585\n","Epoch: 073, Loss: 0.4562\n","Epoch: 074, Loss: 0.4580\n","Epoch: 075, Loss: 0.4581\n","Epoch: 076, Loss: 0.4574\n","Epoch: 077, Loss: 0.4570\n","Epoch: 078, Loss: 0.4589\n","Epoch: 079, Loss: 0.4582\n","Epoch: 080, Loss: 0.4567\n","Epoch: 081, Loss: 0.4568\n","Epoch: 082, Loss: 0.4550\n","Epoch: 083, Loss: 0.4546\n","Epoch: 084, Loss: 0.4567\n","Epoch: 085, Loss: 0.4556\n","Epoch: 086, Loss: 0.4544\n","Epoch: 087, Loss: 0.4564\n","Epoch: 088, Loss: 0.4546\n","Epoch: 089, Loss: 0.4558\n","Epoch: 090, Loss: 0.4547\n","Epoch: 091, Loss: 0.4565\n","Epoch: 092, Loss: 0.4532\n","Epoch: 093, Loss: 0.4534\n","Epoch: 094, Loss: 0.4534\n","Epoch: 095, Loss: 0.4529\n","Epoch: 096, Loss: 0.4536\n","Epoch: 097, Loss: 0.4527\n","Epoch: 098, Loss: 0.4553\n","Epoch: 099, Loss: 0.4536\n","Epoch: 100, Loss: 0.4531\n","Epoch: 101, Loss: 0.4536\n","Epoch: 102, Loss: 0.4543\n","Epoch: 103, Loss: 0.4537\n","Epoch: 104, Loss: 0.4532\n","Epoch: 105, Loss: 0.4510\n","Epoch: 106, Loss: 0.4518\n","Epoch: 107, Loss: 0.4539\n","Epoch: 108, Loss: 0.4519\n","Epoch: 109, Loss: 0.4521\n","Epoch: 110, Loss: 0.4540\n","Epoch: 111, Loss: 0.4495\n","Epoch: 112, Loss: 0.4508\n","Epoch: 113, Loss: 0.4532\n","Epoch: 114, Loss: 0.4506\n","Epoch: 115, Loss: 0.4519\n","Epoch: 116, Loss: 0.4526\n","Epoch: 117, Loss: 0.4493\n","Epoch: 118, Loss: 0.4525\n","Epoch: 119, Loss: 0.4500\n","Epoch: 120, Loss: 0.4541\n","Epoch: 121, Loss: 0.4500\n","Epoch: 122, Loss: 0.4505\n","Epoch: 123, Loss: 0.4483\n","Epoch: 124, Loss: 0.4524\n","Epoch: 125, Loss: 0.4517\n","Epoch: 126, Loss: 0.4502\n","Epoch: 127, Loss: 0.4488\n","Epoch: 128, Loss: 0.4485\n","Epoch: 129, Loss: 0.4502\n","Epoch: 130, Loss: 0.4486\n","Epoch: 131, Loss: 0.4503\n","Epoch: 132, Loss: 0.4521\n","Epoch: 133, Loss: 0.4507\n","Epoch: 134, Loss: 0.4488\n","Epoch: 135, Loss: 0.4481\n","Epoch: 136, Loss: 0.4500\n","Epoch: 137, Loss: 0.4497\n","Epoch: 138, Loss: 0.4489\n","Epoch: 139, Loss: 0.4501\n","Epoch: 140, Loss: 0.4464\n","Epoch: 141, Loss: 0.4490\n","Epoch: 142, Loss: 0.4502\n","Epoch: 143, Loss: 0.4469\n","Epoch: 144, Loss: 0.4490\n","Epoch: 145, Loss: 0.4496\n","Epoch: 146, Loss: 0.4470\n","Epoch: 147, Loss: 0.4496\n","Epoch: 148, Loss: 0.4499\n","Epoch: 149, Loss: 0.4466\n","Epoch: 150, Loss: 0.4487\n","Epoch: 151, Loss: 0.4476\n","Epoch: 152, Loss: 0.4465\n","Epoch: 153, Loss: 0.4492\n","Epoch: 154, Loss: 0.4484\n","Epoch: 155, Loss: 0.4455\n","Epoch: 156, Loss: 0.4475\n","Epoch: 157, Loss: 0.4470\n","Epoch: 158, Loss: 0.4456\n","Epoch: 159, Loss: 0.4456\n","Epoch: 160, Loss: 0.4461\n","Epoch: 161, Loss: 0.4479\n","Epoch: 162, Loss: 0.4473\n","Epoch: 163, Loss: 0.4450\n","Epoch: 164, Loss: 0.4461\n","Epoch: 165, Loss: 0.4453\n","Epoch: 166, Loss: 0.4464\n","Epoch: 167, Loss: 0.4437\n","Epoch: 168, Loss: 0.4469\n","Epoch: 169, Loss: 0.4440\n","Epoch: 170, Loss: 0.4467\n","Epoch: 171, Loss: 0.4454\n","Epoch: 172, Loss: 0.4460\n","Epoch: 173, Loss: 0.4457\n","Epoch: 174, Loss: 0.4452\n","Epoch: 175, Loss: 0.4462\n","Epoch: 176, Loss: 0.4434\n","Epoch: 177, Loss: 0.4444\n","Epoch: 178, Loss: 0.4455\n","Epoch: 179, Loss: 0.4418\n","Epoch: 180, Loss: 0.4444\n","Epoch: 181, Loss: 0.4440\n","Epoch: 182, Loss: 0.4466\n","Epoch: 183, Loss: 0.4422\n","Epoch: 184, Loss: 0.4448\n","Epoch: 185, Loss: 0.4462\n","Epoch: 186, Loss: 0.4437\n","Epoch: 187, Loss: 0.4427\n","Epoch: 188, Loss: 0.4475\n","Epoch: 189, Loss: 0.4461\n","Epoch: 190, Loss: 0.4444\n","Epoch: 191, Loss: 0.4447\n","Epoch: 192, Loss: 0.4435\n","Epoch: 193, Loss: 0.4441\n","Epoch: 194, Loss: 0.4417\n","Epoch: 195, Loss: 0.4443\n","Epoch: 196, Loss: 0.4445\n","Epoch: 197, Loss: 0.4448\n","Epoch: 198, Loss: 0.4439\n","Epoch: 199, Loss: 0.4436\n","Epoch: 200, Loss: 0.4442\n"]}]},{"cell_type":"code","source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vO0M3QtSqfP_","executionInfo":{"status":"ok","timestamp":1717529363417,"user_tz":180,"elapsed":305,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"31061192-29fc-4116-ecde-8b4f0ad7fd36"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7924\n"]}]},{"cell_type":"markdown","source":["## GCN finetuning\n","\n","---"],"metadata":{"id":"Mh5nHGSbsZ2C"}},{"cell_type":"code","source":["from torch_geometric.nn import GCNConv\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n","\n","    def forward(self, x, edge_index):\n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x\n"],"metadata":{"id":"andbLFR_qfnv","executionInfo":{"status":"ok","timestamp":1717529365524,"user_tz":180,"elapsed":340,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Javascript  # Restrict height of output cell.\n","display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n","\n","model = GCN(hidden_channels=16)\n","model = model.to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test():\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n","      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n","      return test_acc\n","\n","\n","for epoch in range(1, 101):\n","    loss = train()\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"8eFpbONuqhbA","executionInfo":{"status":"ok","timestamp":1717529366816,"user_tz":180,"elapsed":908,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"8f1cecaf-1de2-4157-99a4-a9bd49817bef"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 0.6938\n","Epoch: 002, Loss: 0.6827\n","Epoch: 003, Loss: 0.6714\n","Epoch: 004, Loss: 0.6580\n","Epoch: 005, Loss: 0.6450\n","Epoch: 006, Loss: 0.6312\n","Epoch: 007, Loss: 0.6164\n","Epoch: 008, Loss: 0.6007\n","Epoch: 009, Loss: 0.5864\n","Epoch: 010, Loss: 0.5746\n","Epoch: 011, Loss: 0.5636\n","Epoch: 012, Loss: 0.5551\n","Epoch: 013, Loss: 0.5458\n","Epoch: 014, Loss: 0.5384\n","Epoch: 015, Loss: 0.5320\n","Epoch: 016, Loss: 0.5259\n","Epoch: 017, Loss: 0.5213\n","Epoch: 018, Loss: 0.5155\n","Epoch: 019, Loss: 0.5121\n","Epoch: 020, Loss: 0.5102\n","Epoch: 021, Loss: 0.5062\n","Epoch: 022, Loss: 0.5053\n","Epoch: 023, Loss: 0.5025\n","Epoch: 024, Loss: 0.5010\n","Epoch: 025, Loss: 0.4968\n","Epoch: 026, Loss: 0.4982\n","Epoch: 027, Loss: 0.4927\n","Epoch: 028, Loss: 0.4924\n","Epoch: 029, Loss: 0.4917\n","Epoch: 030, Loss: 0.4903\n","Epoch: 031, Loss: 0.4911\n","Epoch: 032, Loss: 0.4905\n","Epoch: 033, Loss: 0.4864\n","Epoch: 034, Loss: 0.4851\n","Epoch: 035, Loss: 0.4842\n","Epoch: 036, Loss: 0.4846\n","Epoch: 037, Loss: 0.4848\n","Epoch: 038, Loss: 0.4836\n","Epoch: 039, Loss: 0.4845\n","Epoch: 040, Loss: 0.4815\n","Epoch: 041, Loss: 0.4815\n","Epoch: 042, Loss: 0.4787\n","Epoch: 043, Loss: 0.4797\n","Epoch: 044, Loss: 0.4771\n","Epoch: 045, Loss: 0.4795\n","Epoch: 046, Loss: 0.4797\n","Epoch: 047, Loss: 0.4797\n","Epoch: 048, Loss: 0.4781\n","Epoch: 049, Loss: 0.4771\n","Epoch: 050, Loss: 0.4774\n","Epoch: 051, Loss: 0.4776\n","Epoch: 052, Loss: 0.4780\n","Epoch: 053, Loss: 0.4779\n","Epoch: 054, Loss: 0.4734\n","Epoch: 055, Loss: 0.4766\n","Epoch: 056, Loss: 0.4768\n","Epoch: 057, Loss: 0.4759\n","Epoch: 058, Loss: 0.4741\n","Epoch: 059, Loss: 0.4731\n","Epoch: 060, Loss: 0.4744\n","Epoch: 061, Loss: 0.4725\n","Epoch: 062, Loss: 0.4733\n","Epoch: 063, Loss: 0.4732\n","Epoch: 064, Loss: 0.4741\n","Epoch: 065, Loss: 0.4735\n","Epoch: 066, Loss: 0.4712\n","Epoch: 067, Loss: 0.4711\n","Epoch: 068, Loss: 0.4720\n","Epoch: 069, Loss: 0.4722\n","Epoch: 070, Loss: 0.4703\n","Epoch: 071, Loss: 0.4712\n","Epoch: 072, Loss: 0.4702\n","Epoch: 073, Loss: 0.4710\n","Epoch: 074, Loss: 0.4686\n","Epoch: 075, Loss: 0.4723\n","Epoch: 076, Loss: 0.4696\n","Epoch: 077, Loss: 0.4680\n","Epoch: 078, Loss: 0.4686\n","Epoch: 079, Loss: 0.4716\n","Epoch: 080, Loss: 0.4676\n","Epoch: 081, Loss: 0.4699\n","Epoch: 082, Loss: 0.4703\n","Epoch: 083, Loss: 0.4684\n","Epoch: 084, Loss: 0.4686\n","Epoch: 085, Loss: 0.4687\n","Epoch: 086, Loss: 0.4659\n","Epoch: 087, Loss: 0.4666\n","Epoch: 088, Loss: 0.4667\n","Epoch: 089, Loss: 0.4668\n","Epoch: 090, Loss: 0.4672\n","Epoch: 091, Loss: 0.4660\n","Epoch: 092, Loss: 0.4634\n","Epoch: 093, Loss: 0.4694\n","Epoch: 094, Loss: 0.4654\n","Epoch: 095, Loss: 0.4657\n","Epoch: 096, Loss: 0.4670\n","Epoch: 097, Loss: 0.4672\n","Epoch: 098, Loss: 0.4682\n","Epoch: 099, Loss: 0.4663\n","Epoch: 100, Loss: 0.4631\n"]}]},{"cell_type":"code","source":["test_acc = test()\n","print(f'Test Accuracy: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29uM7d0eqjVg","executionInfo":{"status":"ok","timestamp":1717529370778,"user_tz":180,"elapsed":315,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"111e1d1c-3477-46a8-ebbf-a17f1bc13d3d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy: 0.7855\n"]}]},{"cell_type":"markdown","source":["## GAT fineutning\n","\n","---"],"metadata":{"id":"XhZEZnbZsYzW"}},{"cell_type":"code","source":["from torch_geometric.nn import GATConv\n","\n","\n","class GAT(torch.nn.Module):\n","    def __init__(self, hidden_channels, heads):\n","        super().__init__()\n","        torch.manual_seed(1234567)\n","        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads)  # TODO\n","        self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes, heads=1)  # TODO\n","\n","    def forward(self, x, edge_index):\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv2(x, edge_index)\n","        return x"],"metadata":{"id":"kzWwop6rsO5O","executionInfo":{"status":"ok","timestamp":1717529373138,"user_tz":180,"elapsed":1,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model = GAT(hidden_channels=16, heads=16)\n","model = model.to(device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n","criterion = torch.nn.CrossEntropyLoss()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dx_S08g9uoAv","executionInfo":{"status":"ok","timestamp":1717529380898,"user_tz":180,"elapsed":418,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"05c04924-4eb0-4c42-d76c-9d224f732d6f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["GAT(\n","  (conv1): GATConv(384, 16, heads=16)\n","  (conv2): GATConv(256, 2, heads=1)\n",")\n"]}]},{"cell_type":"code","source":["def train():\n","      model.train()\n","      optimizer.zero_grad()  # Clear gradients.\n","      out = model(data.x, data.edge_index)  # Perform a single forward pass.\n","      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n","      loss.backward()  # Derive gradients.\n","      optimizer.step()  # Update parameters based on gradients.\n","      return loss\n","\n","def test(mask):\n","      model.eval()\n","      out = model(data.x, data.edge_index)\n","      pred = out.argmax(dim=1)  # Use the class with highest probability.\n","      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n","      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n","      return acc\n","\n","pacience = 0\n","las_val_acc = -1\n","total_pacience = 15\n","best_acc_test = -1\n","best_acc_val = -1\n","for epoch in range(1, 201):\n","    loss = train()\n","    val_acc = test(data.val_mask)\n","    test_acc = test(data.test_mask)\n","\n","    if val_acc < las_val_acc:\n","      pacience += 1\n","    if val_acc > best_acc_val:\n","      best_acc_val =val_acc\n","    if test_acc > best_acc_test:\n","      best_acc_test = test_acc\n","    if pacience == total_pacience:\n","      break\n","    las_val_acc = val_acc\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pI72ppnDsjup","executionInfo":{"status":"ok","timestamp":1717529388779,"user_tz":180,"elapsed":6009,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"c36471a3-50aa-4585-f7c2-e7453697f25f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 001, Loss: 0.6955, Val: 0.5008, Test: 0.5027\n","Epoch: 002, Loss: 0.7046, Val: 0.7024, Test: 0.7028\n","Epoch: 003, Loss: 0.6248, Val: 0.6540, Test: 0.6612\n","Epoch: 004, Loss: 0.6251, Val: 0.6776, Test: 0.6867\n","Epoch: 005, Loss: 0.6099, Val: 0.7208, Test: 0.7282\n","Epoch: 006, Loss: 0.5763, Val: 0.7276, Test: 0.7277\n","Epoch: 007, Loss: 0.5622, Val: 0.7168, Test: 0.7188\n","Epoch: 008, Loss: 0.5662, Val: 0.7236, Test: 0.7254\n","Epoch: 009, Loss: 0.5559, Val: 0.7360, Test: 0.7374\n","Epoch: 010, Loss: 0.5413, Val: 0.7412, Test: 0.7460\n","Epoch: 011, Loss: 0.5304, Val: 0.7428, Test: 0.7452\n","Epoch: 012, Loss: 0.5432, Val: 0.7448, Test: 0.7467\n","Epoch: 013, Loss: 0.5397, Val: 0.7492, Test: 0.7523\n","Epoch: 014, Loss: 0.5287, Val: 0.7488, Test: 0.7535\n","Epoch: 015, Loss: 0.5293, Val: 0.7444, Test: 0.7504\n","Epoch: 016, Loss: 0.5288, Val: 0.7456, Test: 0.7507\n","Epoch: 017, Loss: 0.5278, Val: 0.7504, Test: 0.7549\n","Epoch: 018, Loss: 0.5260, Val: 0.7516, Test: 0.7607\n","Epoch: 019, Loss: 0.5266, Val: 0.7512, Test: 0.7634\n","Epoch: 020, Loss: 0.5236, Val: 0.7508, Test: 0.7636\n","Epoch: 021, Loss: 0.5224, Val: 0.7524, Test: 0.7646\n","Epoch: 022, Loss: 0.5216, Val: 0.7524, Test: 0.7650\n","Epoch: 023, Loss: 0.5198, Val: 0.7464, Test: 0.7614\n","Epoch: 024, Loss: 0.5207, Val: 0.7476, Test: 0.7593\n","Epoch: 025, Loss: 0.5228, Val: 0.7508, Test: 0.7609\n","Epoch: 026, Loss: 0.5219, Val: 0.7560, Test: 0.7646\n","Epoch: 027, Loss: 0.5176, Val: 0.7564, Test: 0.7667\n","Epoch: 028, Loss: 0.5210, Val: 0.7620, Test: 0.7680\n","Epoch: 029, Loss: 0.5206, Val: 0.7612, Test: 0.7676\n","Epoch: 030, Loss: 0.5160, Val: 0.7616, Test: 0.7684\n","Epoch: 031, Loss: 0.5188, Val: 0.7604, Test: 0.7658\n","Epoch: 032, Loss: 0.5193, Val: 0.7552, Test: 0.7631\n","Epoch: 033, Loss: 0.5172, Val: 0.7564, Test: 0.7628\n","Epoch: 034, Loss: 0.5210, Val: 0.7564, Test: 0.7643\n","Epoch: 035, Loss: 0.5179, Val: 0.7596, Test: 0.7674\n","Epoch: 036, Loss: 0.5189, Val: 0.7580, Test: 0.7681\n","Epoch: 037, Loss: 0.5162, Val: 0.7600, Test: 0.7672\n","Epoch: 038, Loss: 0.5160, Val: 0.7584, Test: 0.7674\n","Epoch: 039, Loss: 0.5149, Val: 0.7552, Test: 0.7673\n","Epoch: 040, Loss: 0.5155, Val: 0.7544, Test: 0.7657\n","Epoch: 041, Loss: 0.5158, Val: 0.7564, Test: 0.7656\n"]}]},{"cell_type":"code","source":["print(f'BEST Val: {best_acc_val:.4f}, BEST Test: {best_acc_test:.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"irTYX4Tgsk57","executionInfo":{"status":"ok","timestamp":1717529414368,"user_tz":180,"elapsed":448,"user":{"displayName":"Victor Akihito Kamada Tomita","userId":"12119156415563075107"}},"outputId":"53f33589-58ad-410a-f739-554026e6af66"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["BEST Val: 0.7620, BEST Test: 0.7684\n"]}]}]}